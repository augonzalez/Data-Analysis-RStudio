---
title: "Untitled"
author: "Aura Gonzalez"
date: "1/11/2023"
output: word_document
---

Composici√≥n y Descomposici√≥n de una Serie
El objetivo principal de hacer an√°lisis de series temporales es estudiar a una variable para conocer y entender su pasado, a partir de eso poder hacer predicciones del futuro en base a dicha variable. El an√°lisis de series de tiempo es frecuente en policy makers, ya que este permite hacer una toma de decisiones m√°s acertada. Una serie de tiempo es una secuencia de casos u observaciones en intervalos de tiempo que sigue una estructura regular. Existen diversos ejemplos de series de tiempo, como los siguientes:

‚Ä¢ Las tasas de productividad de una empresa durante los √∫ltimos 5 a√±os.

‚Ä¢ La producci√≥n mensual de az√∫car en toneladas de cierta regi√≥n del pa√≠s, de Enero de1995 hasta diciembre de 1997.

‚Ä¢ La tasa de crecimiento del Producto Interno Bruto de Per√∫ en la d√©cada de 1980.



El formato es ts(vector, start=, end=, frequency= ), donde start y end son los periodos de la primera y la √∫ltima observaci√≥n, respectivamente; frequency indica la frecuencia por unidad de tiempo (1= anual, 4=trimestral, 12=mensual, etc.).


```{r setup, include=FALSE}
gnp.ts = ts(gnp96$gnp96, start=c(1967,1), end=c(2002,2), frequency=4)

print(gnp.ts)
```

## Graficos
Una forma de ver la evoluci√≥n de datos de series de tiempo es generando un gr√°fico de series de tiempo, en este se puede ver la tendencia que ha ido siguiendo cierta variable a trav√©s del tiempo. La funci√≥n plot() permite hacer gr√°ficos para variables de series de tiempo en R.

```{r cars}
plot(gnp.ts, ylab="Producto Nacional Bruto", xlab="Trimestres")
```

Componentes de una Serie de Tiempo
Una serie de tiempo es un dato que se registra a trav√©s del tiempo. Este tipo de datos tiene a su vez componentes propios, suponiendo que son 4 componentes que act√∫an al mismo tiempo, son los que contribuyen a formar las caracter√≠sticas de dicha serie, estos cuatro componentes son la tendencia, la estacionalidad, la ciclicidad y la variaci√≥n irregular o aleatoriedad.

Tendencia
La tendencia de largo plazo es el resultado de factores a largo plazo de la misma. Esta tendencia muestra al patr√≥n que sigue la evoluci√≥n de los datos a trav√©s del tiempo de an√°lisis. Este componente constituye la base del incremento o decremento de la serie de tiempo. Por ejemplo, una serie que representa a la producci√≥n de cierto producto en una empresa, tiene una tendencia que estar√° afectada por diversos factores.

Estacionalidad
Este componente representa la variaci√≥n de los datos en la serie debido a influencias estacionales. Por ejemplo, la variaci√≥n de una serie a√±o a a√±o durante los mismos trimestres que ocurre con la misma fuerza. Un ejemplo m√°s claro puede ser el de una empresa de venta de helados sabe que durante los meses de invierno las ventas de sus productos caer√°n debido al clima en dicha estaci√≥n.

Ciclicidad
Este componente hace referencia a las secuencias o patrones de crecimiento y decrecimiento en la l√≠nea de tendencia que duran m√°s de un a√±o. Este componente es el conjunto de ciclos, de m√°s de un a√±o de duraci√≥n que se producen por diversos factores.

Aleatoriedad o Variaci√≥n Irregular
Este componente se debe a factores de corto plazo, que no pueden preverse y que afectan a la serie. A su vez explica la variabilidad irregular del comportamiento de la serie y es impredecible.

Descomposici√≥n de Series de Tiempo
Antes de hablar de descomposici√≥n de una serie de tiempo se debe entender que existen modelos que explican el comportamiento de una serie de tiempo.

Modelos
Como se pudo ver en la gr√°fica generada, muchas series est√°n dominadas por efectos estacionales y de tendencia. La descomposici√≥n aditiva est√° dada por lo siguiente:

xt=mt+st+zt
Donde t es el tiempo, xt es la variable observada, mt es la tendencia, st es el efecto estacional y zt es el t√©rmino del error. Si el efecto estacional tiende a incrementarse cuando se incrementa la tendencia, un modelo multiplicativo puede ser m√°s apropiado para explicar la serie, este vendr√° dado por:

xt=mt‚àóst+zt
Si la variaci√≥n irregular est√° modelada por un factor multiplicativo y es positivo, un modelo de descomposici√≥n aditivo para loga(ùë•ùë° ) puede ser utilizado:

log(xt)=mt+st+zt

## Descomposici√≥n en R

En R, la funci√≥n decompose() estima los efectos de tendencia y estacional usando el m√©todo de los promedios m√≥viles. Sumado a la funci√≥n plot(), esta produce una figura mostrando la serie original xt y las series de tendencia mt, estacionalidad sty variaci√≥n irregular zt. Asumiendo que la serie sigue un modelo multiplicativo, se tendr√° lo siguiente:



```{r pressure, echo=FALSE}
gnp.decomp = decompose(gnp.ts, type = "mult")
```

Esto generar√° un objeto con cada uno de los componentes de la serie, y tambi√©n se podr√°n generar vectores que contengan a cada uno de estos objetos:

```{r pressure, echo=FALSE}
tend = gnp.decomp$trend
estac = gnp.decomp$seasonal
aleat = gnp.decomp$random
```



Se podr√° tambi√©n hacer un gr√°fico de dicha descomposici√≥n haciendo uso de la funci√≥n plot():

```{r pressure, echo=FALSE}
plot(gnp.decomp)
```

En este ejemplo, el modelo multiplicativo parecer√≠a ser m√°s apropiado que el modelo aditivo, ya que se puede observar un incremento en la serie original y la tendencia. La serie random obtenida de la funci√≥n decompose no es precisamente la realizaci√≥n del proceso aleatorio, sino m√°s bien es una estimaci√≥n de ese proceso.


Existen diversas formas de modelar datos de series de tiempo asumiendo que la serie sigue cierta estructura. Dependiente de la estructura se abordar√°n temas respecto a los modelos estoc√°sticos b√°sicos, como pueden ser los procesos de ruido blanco y random walk. Se ver√° tambi√©n la introducci√≥n a los procesos autorregresivos y el an√°lisis de correlograma para series de tiempo haciendo uso de R.

Procesos Estoc√°sticos Estacionarios
Sea la serie Wt. La estacionariedad sirve para limitar o restringir lavheterogeneidad a trav√©s del tiempo de un proceso estoc√°stico.

De forma estricta la estacionariedad hace referencia a que las funciones de distribuci√≥n conjunta de subconjuntos de un n√∫mero cualquiera ùëõ de una variable no var√≠a por traslaciones en el tiempo para cualquier horizonte ‚Ñé:
f(wt1,wt2,...,wtn)=f(wt1+h,wt2+h,...,wtn+h)‚àÄn,h

En un sentido m√°s amplio un proceso es estacionario si cumple las siguientes condiciones:
E(wt)=Œº,‚àÄt
0<Var(Wt)=œÉ2<‚àû,‚àÄt
Cov(Wt,Wt+k)=Œ≥(k),k=¬±1,¬±2,...,‚àÄt
Cuando el proceso de an√°lisis tiene una distribuci√≥n normal, la estacionariedad en sentido estricto coincidir√° con la de sentido amplio. La estacionariedad se obtiene diferenciando la serie y aplicando logaritmos (Trasformaci√≥n Box-Cox). A partir de las correlaciones se va a poder tomar decisiones sobre cu√°l es el tipo de modelo estad√≠stico m√°s adecuado para dicha serie de tiempo. Cuando se impone la estacionariedad, se reduce n√∫mero de par√°metros, pero todav√≠a queda un n√∫mero elevado de par√°metros por Œ≥k, se necesitar√° una condici√≥n m√°s, que es la ergodicidad (consistencia). Cuando se quiere hacer pruebas de hip√≥tesis y an√°lisis de los par√°metros, se va a dar con que a medida que aumenta el n√∫mero de observaciones, aumentar√° el n√∫mero de par√°metros desconocidos. La condici√≥n de ergodicidad ser√° la siguiente: La correlaci√≥n serial entre variables de una serie disminuye a medida que nos alejamos en el tiempo.
limx‚Üí‚àûŒ≥k=0

Proceso de Ruido Blanco
Un proceso de ruido blanco o White noise, es el modelo m√°s simple de series temporales. Este modelo trata de una serie puramente aleatoria y se representa por ùëéùë° .Una serie ser√° de ruido blanco cuando:

La esperanza es constante, , es igual a cero.
Cov(at,at+k)=0 para todo k‚â†0
Este proceso se trata b√°sicamente de uno en el cual todas sus variables son independientes. Un ejemplo de un proceso de ruido blanco son los n√∫meros de una loter√≠a:

‚Ä¢ Cada n√∫mero es independiente del anterior. ‚Ä¢ No hay dependencia entre el pasado y el futuro.

Es decir, la variable tiempo no influye, ya que no hay dependencia, esto es totalmente impredecible. Un modelo de series temporales adopta la siguiente expresi√≥n:
yt=f(pasado)+at=f(yt‚àí1,tt‚àí2,...)+at
En ùë° ‚àí 1 la funci√≥n es conocida, pero ùëéùë° ser√° desconocida, ya que es una variable aleatoria que no se puede predecir.

Simulaci√≥n en R
Un proceso de ruido blanco puede ser usado en datos simulados. Las series de tiempo simuladas son llamadas series sint√©ticas para distinguirlas de las series hist√≥ricas observadas. La simulaci√≥n es √∫til por muchas razones, esta puede ser usada para generar escenarios futuros y construir intervalos de confianza para par√°metros del modelo. En R, la simulaci√≥n es un procedimiento sencillo, la mayor√≠a de las distribuciones estad√≠sticas son simuladas usando una funci√≥n que tiene el nombre abreviado de la distribuci√≥n con un prefijo ‚Äòr‚Äô (de ‚Äúrandom‚Äù). Por ejemplo, rnorm(100) es usada para simular 100 valores con distribuci√≥n normal, que es equivalente a simular un serie de ruido blanco Gaussiano de longitud (100). La sintaxis para hacer simulaciones es la siguiente:


```{r pressure, echo=FALSE}
set.seed(1)
w = rnorm(100)

```


Podemos visualizar la serie haciendo grafic√°ndola:



```{r pressure, echo=FALSE}
plot(w, type="l")

```


La funci√≥n set.seed es usada para dar un punto de salida (o seed) en simulaciones, que garantiza as√≠ la reproducci√≥n de la simulaciones. Si esta funci√≥n no es indicada, una muestra diferente de datos ser√° simulada, y las propiedades estad√≠sticas cambiar√°n.


Proceso Random Walk

El proceso random walk viene dado por la siguiente expresi√≥n:
yt=yt‚àí1+at
Donde at es un proceso de ruido blanco, es decir, entra en el proceso como shock aleatorio que se incorpora en cada momento. Este proceso tiene ra√≠z unitaria, (coeficiente de yt‚àí1), por lo tanto, la serie muestra un perfil o una tendencia evolutiva, por lo que se entender√° que no es estacionaria. Como ejemplo de este tipo de procesos, podemos notar a diferentes variables econ√≥micas en mercado eficientes, en este tipo de mercados se entiende que con un elevado n√∫mero de agentes con informaci√≥n completa los valores presentes de las series se adaptar√°n a la informaci√≥n disponible, es decir el precio de cierto bien en el periodo t‚àí1 ser√° el mismo que el de futuro.

Simulaci√≥n en R
Ya se ha visto en el punto anterior que es muy √∫til el uso de simulaciones en series de tiempo. Estas simulaciones permiten observar las caracter√≠sticas principales del modelo en gr√°ficas, como la data hist√≥rica tiene propiedades similares, el modelo puede ser seleccionado como un candidato potencial. Los siguientes comandos pueden ser usados para modelar un proceso aleatorio en una serie de nombre y:


```{r pressure, echo=FALSE}
y <- w <- rnorm(1000)

```

Esta primera l√≠nea de comandos indica que se quiere crear dos series con 1000 observaciones, que siguen una distribuci√≥n normal, la primera serie es la serie en s√≠, y w es el t√©rmino que representa al ruido blanco.


```{r pressure, echo=FALSE}
for (t in 2:1000) y[t] <- y[t - 1] + w[t]
```


Esto indica que la serie tendr√° la forma que se especific√≥ para un proceso random walk.Se puede hacer el gr√°fico de dicha serie, al igual que con el ruido blanco se usar√° la funci√≥n plot().

```{r pressure, echo=FALSE}
plot(y, type = "l")
```


Este es el gr√°fico generado por el proceso simulado de random walk o paseo aleatorio. Exhibe una tendencia a la baja, sin embargo, esto es puramente estoc√°stico debido a la alta correlaci√≥n serial.

La Autocorrelaci√≥n
Funci√≥n Autocovarianza
Es un proceso gaussiano estacionario donde la media y la varianza son independientes y la covarianza de dos variables depender√° del desfase temporal de k.

Cov(yt,yt+k=Œ≥(k)+Œ≥k para k=1,2,3,..., para todo t
Œ≥k=E[(yt‚àíŒº)(yt+k+Œº)]
La funci√≥n (k) es llamada la funci√≥n de autocovarianzas del proceso. Y tiene las siguientes propiedades:

Œ≥0>0yaqueeslavarianza
yk=y‚àík es decir la covarianza entre una variable y otra que presenta n desfase de k per√≠odos, ya sea hacia el pasado o hacia el futuro.
|yk|‚â§y0
Funci√≥n de Autocorrelaci√≥n
La funci√≥n de autocovarianzas va a depender de la unidad de medida empleada. La correlaci√≥n estar√° definida por la siguiente expresi√≥n:

Cor(yt,yt+k)=p(k)=Œ≥kŒ≥o, para k=1,¬±2,¬±3,... para todo t

La funci√≥n p(k)se denomina funci√≥n de autocorrelaci√≥n, es una funci√≥n ibre de las unidades de medida de las variables, esta funci√≥n mide la dependencia lineal existente entre las variables. Esta funci√≥n tiene las siguientes propiedades:

p0 = 1
pk=p‚àík es decir, la correlaci√≥n entre una variable y otra que presenta un desfase de k periodos, ya sea hacia el pasado o futuro.
|pk|‚â§1
En un proceso estacionario gaussiano toda la dependencia entre las variables viene recogida por la FAC (Funci√≥n de autocorrelaci√≥n). La funci√≥n de autocovarianza y la de autocorrelaci√≥ puede ser estimada de una serie de tiempo por sus equivalentes muestrales, ese estimador de la funci√≥n de autocovarianza, ck, es el siguiente:
ck=1k‚àët=1n‚àík(yt‚àíy¬Ø)(yt+k‚àíy¬Ø)
Se debe notar que la autocovarianza en el rezago 0, es la varianza calculada con denominador n. El estimador de la funci√≥n de autocorrelaci√≥n st√° definido por:
rk=ckc0
#### C√°lculo en R

La autocorrelaci√≥n e la variable y se almacenan en el vector acf(y)$asf, con el rezago k localizado en acf(x)$acf[k+1]. Por ejemplo, de acuerdo a la base de datos de lo trabajador anteriormente, la autocorrelaci√≥n el rezago 1 para gnp.ts es:



```{r pressure, echo=FALSE}
acf(gnp.ts)$acf[2]
```

El primer valor , acf(x)$acf[k+1], es r0 y es igual a 1. Para obtener as autocovarianzas se debe a√±adir un argumento a la funci√≥n acf(), a autocovarianza para el rezago 1 est√° dado por:

```{r pressure, echo=FALSE}
acf(gnp.ts, type = c("covariance"))$acf[2]
```

El Correlograma
Por defecto, a funci√≥n acf(), produce gr√°fico de rk contra k que es llamada correlograma. Por ejemplo, si escribimos la sintaxis acf(gnp.ts)obtendremos el correlograma para dicha serie.

```{r pressure, echo=FALSE}
acf(gnp.ts)
```

De este gr√°fico se puede entender que:

El eje x da el rezago k y el eje y da la autocorrelaci√≥n rk en cada rezago. Si pk=0, la distribuci√≥n muestral de rk es aproximadamente normalm con una media de ‚àí1/n y una varianza de 1/n. Las l√≠neas del correlogama se grafican en:
‚àí1n¬±2n‚Äæ‚àö
Si rk cae afuera de esas l√≠neas, se tiene evidencia contra la hip√≥tesis nula pk=0al 5% de significancia. Sin embargo se debe tener cuidado al interpretar la hip√≥tesis nula. En primer lugar, si pk es igual a 0 en el rezago k , se espera que el 5% de los estimados caiga fuera de las l√≠neas. En segundo lugar, os rk est√°n correlacionados, por lo tanto, si uno cae fuera de las l√≠neas, los coeficientes cercanos est√°n m√°s probables a ser estad√≠sticamente singificativos.

Modelos Autoregresivos
Una serie yt es un proceso autoregresivo de orden p, abreviado como AR(P), si:

yt=Œ±1yt‚àí1+Œ±2yy‚àí2+...+Œ±pyt‚àíp+Wt
Donde Wt es el ruido blanco y Œ±i son los par√°metros del modelo con Œ±p‚â†0 para un proceso de orden p. La ecuaci√≥n anterior puede ser expresada como un polinomio de orden p en t√©rmino del operador de rezago:

Œ∏p(B)yt=(1‚àíŒ±1B‚àíŒ±2B2‚àí...‚àíŒ±pBp)yt=Wt
Donde se debe notar lo siguiente:

El proceso de random walk es un caso especial de un modelo autoregresivo AR(1) con Œ±1=1.
El modelo es una regresi√≥n de yt en sus t√©rminos pasados de la misma serie, de ah√≠ el nombre de ‚Äúautoregesivo‚Äù.
La predicci√≥n en el tiempo t est√° dada por: yt¬Ø=Œ±1yt‚àí1+Œ±2yt‚àí2+...+Œ±pyt‚àíp
Los par√°metros del modelo pueden ser estimados por medio de la minimizaci√≥n de la suma de los cuadrados de los errores (MCO).
Correlogama de un Proceso AR(1)
La funci√≥n de autocorrelaci√≥n de un proceso AR(1) viene dada por:

pk=Œ±k(k‚â•0)
Donde |Œ±|<1. As√≠, el correlograma decrece a 0 o m√°s r√°pidamente para los Œ± peque√±os. El siguiente ejemplo muestra dos correlogramas para valores positivos y negativos de Œ±, respectivamente. La sintaxis siguiente mostrar√° el siguiente gr√°fico:

rho <- function(k, alpha) alpha^k
layout(1:2)
plot(0:10, rho(0:10, 0.7),type = "b")
plot(0:10, rho(0:10, -0.7), type = "b")
 Se puede ver que hay decaimiento r√°pido hacia 0.

Autocorrelaci√≥n Parcial
La autocorrelaci√≥n parcial en el rezago k es la correlaci√≥n que resulta de quitar el efecto de cualquier correlaci√≥n debido a los t√©rminos de los rezagos m√°s cortos. Por ejemplo, la autocorrelaci√≥n parcial de un proceso *AR(1) ser√° 0 para todos los rezagos m√°s grande que 1. En general, la autocorrelaci√≥n parcial en el rezago k es el kvo coeficiente de un modelo AR(k), si el proceso subyacente es un AR(p)**, entonces los coeficientes Œ±k ser√°n cero para todo k>p.

Simulaci√≥n en R
Un AR(1) puede ser simulado en R se la siguiente manera:

set.seed(1)
y <- w <- rnorm(100)
for (t in 2:100) y[t] <- 0.7 * y[t-1] + w[t]
layout(1:3)
plot(y, type = "l"); acf(y); pacf(y)
 Este gr√°fico es un proceso AR(1), que es yt=0.7yt‚àí1+wt. N√≥tese que en la autocorrelaci√≥n parcial solo el primer rezago es significativo, que es usualmente caso cuando el proceso subyacente es un AR(1).

Modelaci√≥n de un AR(1) en R
UnAR(p) puede ser estimado en R haciendo uso de la funci√≥n ar(). De acuerdo a la serie simulada anteriormente, es estimar√° el modelo AR(1) de la siguiente manera:

y.ar = ar(y, method = "ols")
La sintaxis indica que se quiere estimar un modelo autorregresivo para el objeto y usando el m√©todo de estimaci√≥n de m√≠nimos cuadrados, aunque esto puede variar, ya que esta funci√≥n tambi√©n ofrece la estimaci√≥n por m√°xima verosimilutd. Se puede observar el orden del modelo autorregresivo para dar raz√≥n que es de orden 1:

```{r pressure, echo=FALSE}
y.ar$order
```

Para conocer el t√©rmino autorregresivo se debe seguir la siguiente especificaci√≥n:

```{r pressure, echo=FALSE}
y.ar$ar
```

Regresi√≥n en Series de Tiempo
Las tendencias en series de tiempo se clasifican como estoc√°sticas o determin√≠sticas. Se puede considerar a una tendencia como estoc√°stica cuando muestra cambios inexplicables en su direcci√≥n, y se atribuye las tendencias transitorias aparentes a la alta correlaci√≥n serial con el t√©rmino de error. En contraste a esto, una tendencia ser√° aleatoria cuando se tiene un modelo con explicaci√≥n aceptable de la tendencia de dicha serie.

Modelos Lineales
Un modelo para una serie de tiempo (xt:t=1,...,n) es lineal si se puede expresar como:
xt=Œ±0+Œ±1Œº1,t+Œ±2Œº2,t+...+Œ±mŒºm,t+Zt
Donde ùë¢ùëñ,ùë° es el valor de la i-√©sima variable predictora en el tiempo ùë°, ùëßùë° es el error en el tiempo ùë°, y ùõº0, ùõº1, ‚Ä¶ , ùõºùëö son par√°metros del modelo, que pueden ser estimados por m√≠nimos cuadrados. Un ejemplo del modelo lineal es la funci√≥n polinomial de ùë° de orden ùëù:
xt=Œ±0+Œ±1t+Œ±2t2+...+Œ±ptp+Zt
Las variables predictoras pueden ser escritas como ùë¢ùëñ,ùë° = ùë°ùëñ (ùëñ = 1, ‚Ä¶ , ùëù). El t√©rmino lineal es una referencia de la sumatoria de los par√°metros del modelo, cada una por la variable predictora.

Estacionariedad
Los modelos lineales para series de tiempo son estacionarios cuando incluyen funciones de tiempo. La diferenciaci√≥n puede transformar series no estacionarias con tendencia determin√≠stica en series estacionarias. Por ejemplo, si la serie de tiempo ùë•ùë° esta dada por una funci√≥n de l√≠nea recta m√°s el ruido blanco xt=Œ±0+Œ±1t+Zt, entonces la diferencia de primer orden estar√° dada por:
‚ñΩXt=Xt‚àíXt‚àí1=Zt‚àíZt‚àí1+Œ±1
Asumiendo que la serie del t√©rmino de error Zt es estacionaria, la serie ‚ñΩXt es estacionaria ya que no es una funci√≥n de t.

Simulaci√≥n
En regresiones de series de tiempo, es com√∫n que la serie de error Zt est√© autocorrelacionada. En el siguiente c√≥digo se simular√° y se graficar√° una serie con tendencia de l√≠nea recta (50 + 3ùë°) con errores autocorrelacionados.El c√≥digo indica que se crear√° un modelo del siguiente tipo xt=50+3t+Zt donde Zt sigue un proceso AR(1)Zt=0.8Zt‚àí1+Wt y Wt es el t√©rmino de ruido blanco con desviaci√≥n est√°ndar igual a 20.

```{r pressure, echo=FALSE}
set.seed(1)
z = w = rnorm(100, sd=20)
for (t in 2:100) z[t] = 0.8 *z[t-1] + w[t]
Time = 1:100
x = 50+3*Time+z
plot(x, xlab = "time", type = "l")
```

Ajuste del Modelo
Los modelos lineales se suelen ajustar usualmente minimizando la suma de los cuadrados del error ‚àëZ2t=‚àë(xt‚àíŒ±0‚àíŒ±1Œº1‚àí...‚àíŒ±mŒºm,t)2, que puede ser estimado en R con la funci√≥n lm.



```{r pressure, echo=FALSE}
x.lm = lm(x~Time)
coef(x.lm)
```


```{r pressure, echo=FALSE}
sqrt(diag(vcov(x.lm)))
```

En la sintaxis indicada, se extraen los par√°metros del modelo estimado con la funci√≥n coef(). Se debe notar, como se esperaba, que los valores estimados son cercanos a los par√°metros especificados, 50 para el intercepto y 3 para el coeficiente. Los errores est√°ndar se extraen sacando la ra√≠z cuadrada a los elementos de la diagonal de la matriz de varianzas y covarianzas (vcov). Despu√©s de ajustar el modelo de regresi√≥n, se deben considerar diversos gr√°ficos de diagn√≥stico. En el caso de la regresi√≥n de series de tiempo, un diagnostico importante es el gr√°fico de correlograma de los residuos, haciendo uso de la funci√≥n de autocorrelaci√≥n y la funci√≥n de autocorrelaci√≥n parcial, de la siguiente manera:



```{r pressure, echo=FALSE}
acf(resid(x.lm))
```


```{r pressure, echo=FALSE}
pacf(resid(x.lm))
```

Modelos Lineales con Variables Estacionales
Ya que las series de tiempo son observaciones medidas de forma secuencial en el tiempo, suelen presentarse efectos estacionales en los datos.

Variables aditivas de indicadores estacionales
Suponga que una serie de tiempo contiene s estaciones. Por ejemplo, para una serie de tiempo medida para cada mes del calendario, s=12, as√≠ como las series semestrales, con s=2. Un modelo con indicador estacional para una serie de tiempo (xt:t=1,...,n) que contiene s estaciones y una tendencia mt est√° dada por:
xt=mt+st+zt
Donde st=Œ≤i cuando t cae en la i-√©sima estaci√≥n (t=1,‚Ä¶,n ; i = 1,‚Ä¶,s) y zt es el error residual de la serie, que puede estar autocorrelacionada . Este modelo toma la misma forma que un modelo de descomposici√≥n aditivo. La ecuaci√≥n anterior no muestra en mt un t√©rmino constante, es decir mt puede ser un polinomio de orden p con par√°metros Œ±1,...,Œ±p. Esta ecuaci√≥n es entonces equivalente a una tendencia polin√≥mica en donde al t√©rmino constante depende de la estaci√≥n, por lo tanto, los par√°metros Œ≤1,...,Œ≤s corresponden a los s posibles t√©rminos constantes. La ecuaci√≥n puede ser escriba como:
xt=mt+Œ≤1+(t‚àí1)mod s +Zt
Por ejemplo, con una serie de tiempo observada para cada mes del calendario comenzando con t=1 como enero, un indicador estacional con tendencia de l√≠nea recta est√° dado por:

Ecuaci√≥n
Ecuaci√≥n

Los par√°metros del modelo en esta ecuaci√≥n pueden ser estimados por MCO o M√≠nimos Cuadrados Generalizados, tratando al t√©rmino estacional St como un factor. En R, la funci√≥n factor() puede ser aplicada para los √≠ndices estacionales extra√≠dos usando la funci√≥n cycle(). Con la base de datos de sesiones pasadas sobre el producto nacional bruto trimestral, se puede estimar el modelo de regresi√≥n estacional de la siguiente manera:


```{r pressure, echo=FALSE}
gnp.ts = ts(gnp96$gnp96, start=c(1967, 1), end=c(2002,2), frequency = 4)
gnpt = window(gnp.ts, start = 1990)
```

Se est√° indicando la extracci√≥n de una parte de la base total, que ser√° desde 1990, luego se deben extraer los √≠ndices estacionales con la funci√≥n cycle() y el tiempo con la funci√≥n time(); luego de eso se har√° la regresi√≥n del modelo indicando que los √≠ndices estacionales son una variable de factor, haciendo uso de la funci√≥n del mismo nombre (factor).



```{r pressure, echo=FALSE}
Seas <- cycle(gnpt)
Time <- time(gnpt)
temp.lm <- lm(gnpt ~ 0 + Time + factor(Seas))
```


Los coeficientes se observar√°n usando la funci√≥n coef(), los ceros usados en la f√≥rmula indica que el modelo no tiene intercepto.

```{r pressure, echo=FALSE}
coef(temp.lm)
```

Modelos No Lineales
Los modelos lineales son aplicables en cierto rango. Sin embargo, para algunas series de tiempo puede ser m√°s apropiado ajustar modelos no lineales de forma directa en lugar de usar logaritmos o usar aproximaciones polin√≥micas. Por ejemplo, si se conoce que una serie est√° derivada de un proceso no lineal, puede que est√© basada en una ley determin√≠stica en la ciencia, es por eso que ser√° mejor usar esta informaci√≥n en la formulaci√≥n del modelo y ajustar un modelo no lineal directamente a los datos. Para estimar modelo no lineales en R, se debe hacer usar la funci√≥n nls(), que hace referencia a la estimaci√≥n por m√≠nimos cuadrados no lineales. Se ha visto que se puede estabilizar la varianza aplicando logaritmos naturales a una serie. Sin embargo, usar logaritmos puede presentar dificultades cuando la serie presenta valores negativos, ya que el logaritmo de un valor negativo no est√° definido. Una manera de evitar dicho problema es a√±adiendo una constante a todos los t√©rminos de la serie, entonces, si Xt es una serie que contiene valores negativos, a√±adiendo co tal que co>max(xt) y tomando los logaritmos se produce una serie transformada (log(co+xt)) que est√° definida para todo t. Un modelo lineal puede entonces ser ajustado para producir el modelo:
xt=‚àíco+eŒ±0+Œ±1t+zt
Donde Œ±0 y Œ±1 son par√°metros del modelo y Zt es una serie residual que puede estar autocorrelacionada. La principal dificultad de la ecuaci√≥n es que co debe ser estimado como otro par√°metro del modelo, mientras que en la pr√°ctica el investigador suele elegir un valor arbitrario que satisface la condici√≥n (co>max(‚àíxt)). El modelo puede ser estimado por m√≠nimos cuadrados no lineales, de la siguiente manera:
xt=‚àíco+eŒ±0+Œ±1t+Zt
## Simulaci√≥n y ajuste de series no lineales

Como los modelos no lineales son generalmente ajustados cuando se conoce la funci√≥n no lineal subyacente, se simular√° una serie basada en la √∫ltima ecuaci√≥n, con co=0 y se comparar√°n los par√°metros de la estimaci√≥n usando la funci√≥n nls().

```{r pressure, echo=FALSE}
set.seed(1)
w = rnorm(100, sd = 10)
z = rep(0 , 100)
for (t in 2:100) z[t] = 0.7 * z[t-1] + w[t]
Time <- 1:100
f <- function(x) exp(1 + 0.05 * x)
x <- f(Time) + z
plot(x, type = "l")
abline(0, 0)
```


```{r pressure, echo=FALSE}
x.nls <- nls(x ~ exp(alp0 + alp1 * Time), start = list(alp0 = 0.1,
alp1 = 0.5))
summary(x.nls)$parameters
```

Pron√≥sticos para Regresiones
Un pron√≥stico es una predicci√≥n para el futuro. En el contexto de regresi√≥n de series de tiempo, un pron√≥stico implica la extrapolaci√≥n del modelo ajustado evaluando en √©l la funci√≥n para nuevas series de tiempo. El problema principal con esta aproximaci√≥n es que las tendencias que se presentan en las series ajustadas pueden cambiar en el futuro. Por lo tanto, es mejor pensar en un pron√≥stico de una regresi√≥n como un valor condicional esperado en tendencias pasadas que contin√∫an en el futuro.

Predicci√≥n en R
La funci√≥n gen√©rica para hacer predicciones en R es predict(). Esta funci√≥n toma esencialmente un modelo ajustado y datos nuevos como par√°metros. La clave para usar esta funci√≥n con un modelo de regresi√≥n es asegurarse que la nueva data est√° debidamente definida y etiquetada en un data.frame.

```{r pressure, echo=FALSE}
Seas <- cycle(gnpt)
Time <- time(gnpt)
temp.lm <- lm(gnpt ~ 0 + Time + factor(Seas))
#
new.t <- time(ts(start = 1990, end = c(2002, 2), fr = 4))
pron = ts(predict(temp.lm, new.t), st = 1990, fr=4)
ts.plot(gnp.ts, pron, lty = 1:2)
```

La funci√≥n Forecast
El n√∫mero de valores para los periodos siguientes que se desea generar es 8 en este caso. El modelo creado con tslm contiene la tendencia y el factor estacional.

```{r pressure, echo=FALSE}
library(forecast)
mod <- tslm(gnpt ~ 0 + trend + season)
fc = forecast(mod, h = 8)
fc
```


```{r pressure, echo=FALSE}
fc

View(milkpercow)
colnames(milkpercow) = c("mes", "milk")
```


```{r pressure, echo=FALSE}
milk.ts = ts(milkpercow$milk, start=c(1962, 1), end=c(1975,12), frequency = 12)

plot(milk.ts, xlab = "Tiempo" , main = "Produccion mensual de leche")
```
